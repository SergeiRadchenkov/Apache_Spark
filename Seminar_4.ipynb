{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmDx0aoWiotA"
      },
      "outputs": [],
      "source": [
        "!pip install pyspark >> None"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum\n",
        "from pyspark.sql.functions import *\n",
        "import pyspark.sql.functions as F"
      ],
      "metadata": {
        "id": "5x37G1yci76h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 1. Вывод данных в консоль"
      ],
      "metadata": {
        "id": "rqLUlvshjQ-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем \"CountElements\".\n",
        "2. Читаем поток данных с использованием источника \"rate\", который автоматически генерирует данные. Этот источник используется для тестирования и отладки, поскольку он позволяет легко создавать потоки данных без необходимости подключения к внешним системам.\n",
        "3. Записываем полученные данные в консоль с использованием режима вывода \"append\". В этом режиме в консоль будут выводиться только новые строки, полученные в каждом микро-пакете данных.\n",
        "4. Запускаем поток обработки данных и ожидает его завершения."
      ],
      "metadata": {
        "id": "NVXgofYDjsl1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"CountElements\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "query = df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination(20)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "Mq8po2-ij3tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 2. Фильтрация чисел (четные)"
      ],
      "metadata": {
        "id": "cnmatHcbm62V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем приложения \"FilterEvenNumbers\". Это необходимо для инициализации Spark и подготовки к выполнению операций с данными.\n",
        "\n",
        "2. Читаем потоковые данные из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и демонстрации обработки потоковых данных.\n",
        "\n",
        "3. Фильтруем полученные данные, оставляя только те строки, где значение поля \"value\" делится на 2 без остатка, то есть числа, являющиеся четными.\n",
        "\n",
        "4. Записываем отфильтрованные данные обратно в консоль с использованием режима вывода \"append\". Это означает, что при каждом обновлении данных в потоке, новые данные будут добавлены к уже существующим, не удаляя предыдущие записи.\n",
        "\n",
        "5. Запускаем потоковую запись и ожидает завершения работы запроса в течение 5 секунд. Это позволяет увидеть результаты обработки данных в консоли в течение этого времени.\n"
      ],
      "metadata": {
        "id": "-poz7VDbnBBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"FilterEvenNumbers\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_even = df.filter(\"value % 2 == 0\")\n",
        "query = df_even.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "HrU5a6B6nFZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 3. Сгруппировать по значениям и посчитать количество\n"
      ],
      "metadata": {
        "id": "NO6AnSgLoYER"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем приложения \"GroupByValue\". Это необходимо для работы с Spark и его функциональностью, включая потоковую обработку данных.\n",
        "\n",
        "2. Читаем потоковые данные из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с постоянной скоростью, что удобно для тестирования и демонстрации потоковой обработки.\n",
        "\n",
        "3. Группируем полученные данные по столбцу \"value\" и подсчитывает количество записей для каждого уникального значения в этом столбце. Это позволяет агрегировать данные по определенному критерию.\n",
        "\n",
        "4. Запускаем потоковую запись результатов агрегации в консоль с использованием режима вывода \"update\". Режим \"update\" означает, что в консоль будут выводиться только обновленные результаты агрегации, а не полный набор данных при каждом обновлении. Это делает вывод более читаемым и эффективным, особенно когда данные постоянно обновляются.\n",
        "\n",
        "5. Ожидаем завершения потоковой обработки в течение 5 секунд. Это означает, что поток будет работать в течение этого времени, а затем завершится.\n"
      ],
      "metadata": {
        "id": "Jf23NTJNogi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"GroupByValue\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_grouped = df.groupBy(\"value\").count()\n",
        "query = df_grouped.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "T58ystLkohkw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 4. Вычислить сумму значений"
      ],
      "metadata": {
        "id": "BKg7xbMOo0F7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем экземпляр SparkSession с именем приложения \"SumValues\". Этот шаг необходим для инициализации Spark и подготовки среды для выполнения операций с данными.\n",
        "\n",
        "2. Читаем поток данных из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и отладки. В данном случае, данные будут генерироваться автоматически без необходимости подключения к внешним источникам данных.\n",
        "\n",
        "3. Выполняем агрегацию данных, вычисляя сумму значений в столбце \"value\" и сохраняя результат в новом столбце \"total\". Это делается с помощью метода selectExpr, который позволяет выполнять SQL-подобные выражения для трансформации данных.\n",
        "\n",
        "4. Записываем результаты агрегации в консоль с использованием формата \"console\". Это позволяет наблюдать за изменениями в реальном времени, так как данные будут выводиться в консоль.\n",
        "\n",
        "5. Запускаем запрос на потоковую обработку данных с использованием режима вывода \"update\". В этом режиме, при каждом обновлении данных, будет выводиться только текущее состояние агрегации, что позволяет видеть актуальную сумму значений.\n",
        "\n",
        "6. Ожидаем завершения обработки потока данных в течение 5 секунд с помощью метода awaitTermination. Это означает, что приложение будет работать в течение этого времени, обрабатывая поступающие данные и выводя результаты в консоль."
      ],
      "metadata": {
        "id": "fBL6uYTco2nk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"SumValues\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_sum = df.selectExpr(\"sum(value) AS total\")\n",
        "query = df_sum.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "wrbycGhNo5yt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Задание 5. Найти максимальное значение"
      ],
      "metadata": {
        "id": "4zFB-pjQq8bI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем приложения \"MaxValue\" с помощью SparkSession.builder.appName(\"MaxValue\").getOrCreate(). Это необходимо для работы с Spark и его функциональностью, включая структурированный потоковый анализ.\n",
        "\n",
        "2. Читаем поток данных с использованием источника \"rate\". Источник \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и разработки. В данном случае, данные читаются без дополнительных параметров, что означает использование значений по умолчанию для генерации данных.\n",
        "\n",
        "3. Выполняем агрегацию данных, вычисляя максимальное значение поля 'value' в каждом микро-батче данных. Это достигается с помощью метода agg(f.max('value')), где f - это ссылка на модуль pyspark.sql.functions, который предоставляет функции для работы с данными.\n",
        "\n",
        "4. Записываем результаты агрегации в консоль с использованием writeStream.outputMode(\"update\").format(\"console\").start(). Здесь outputMode(\"update\") указывает, что при каждом обновлении данных в потоке, результаты агрегации будут выводиться в консоль. format(\"console\") указывает, что вывод должен быть направлен в консоль, а не в файл или базу данных.\n",
        "\n",
        "5. Ожидаем завершения потокового запроса с помощью query.awaitTermination(5), что означает, что поток будет работать в течение 5 секунд, после чего будет выполнено завершение работы."
      ],
      "metadata": {
        "id": "LbhjURIQrB6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"MaxValue\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_max = df.agg(F.max('value'))\n",
        "query = df_max.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "4nBFfog7rG2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 6. Вычислить скользящее окно по значению"
      ],
      "metadata": {
        "id": "abV8gawBrWSz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем \"SlidingWindow\".\n",
        "2. Читаем потоковые данные из источника \"rate\", который генерирует данные с постоянной скоростью.\n",
        "3. Группируем эти данные по временным окнам, размером в 10 минут, и подсчитывает количество записей в каждом окне.\n",
        "4. Записываем результаты обработки в консоль в режиме обновления (outputMode(\"update\")), что означает, что в консоль будут выводиться только обновленные агрегированные результаты.\n",
        "5. Запускаем потоковую запрос и ожидает его завершения в течение 5 секунд."
      ],
      "metadata": {
        "id": "WEkWrFoBrYsd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"SlidingWindow\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "df_windowed = df.groupBy(window(\"timestamp\", \"10 minutes\")).count()\n",
        "query = df_windowed.writeStream.outputMode(\"update\").format(\"console\").start()\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "VoxJ27XgrdUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 7. Соединение потоков данных"
      ],
      "metadata": {
        "id": "Ng2pC9T6jTEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем сессию Spark с именем приложения \"JoinStreams\".\n",
        "2. Читаем два потока данных, используя формат \"rate\", который генерирует данные с определенной скоростью. По умолчанию, каждый поток будет генерировать одну строку в секунду с полями timestamp и value.\n",
        "3. Соединяем два потока данных по полю value. Это означает, что для каждой пары строк из двух потоков, где значение поля value совпадает, будет создана новая строка в результате соединения.\n",
        "4. Записываем результат соединения в консоль с использованием режима вывода \"append\". Это означает, что при каждом обновлении данных в потоках, новые строки будут добавляться к выводу, не удаляя предыдущие строки.\n",
        "5. Запускаем запрос на потоковую обработку и ожидает его завершения в течение 5 секунд.\n"
      ],
      "metadata": {
        "id": "_baXgwNQjZ3e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"JoinStreams\").getOrCreate()\n",
        "df1 = spark.readStream.format(\"rate\").load()\n",
        "df2 = spark.readStream.format(\"rate\").load()\n",
        "df_joined = df1.join(df2, \"value\")\n",
        "query = df_joined.writeStream.outputMode(\"append\").format(\"console\").start()\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "ZmETfvhxjj2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Задание 8. Запись данных в файл"
      ],
      "metadata": {
        "id": "jK6-OVnijn3K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создаем экземпляр SparkSession с именем приложения \"WriteToFile\". SparkSession является точкой входа для работы с Spark и предоставляет API для работы с DataFrame и DataSet.\n",
        "\n",
        "2. Читаем потоковые данные с использованием формата \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и отладки потоковых приложений. В данном случае, данные будут генерироваться без дополнительных параметров, что означает использование значений по умолчанию.\n",
        "\n",
        "3. Записываем полученные потоковые данные в формате Parquet в указанный каталог \"output/\". Parquet - это эффективный формат хранения данных, который обеспечивает высокую производительность и эффективное сжатие.\n",
        "\n",
        "4. Запускаем потоковую запись данных и ожидает завершения работы запроса в течение 5 секунд. Метод awaitTermination блокирует выполнение программы до тех пор, пока потоковая запись не будет остановлена или не произойдет ошибка.\n"
      ],
      "metadata": {
        "id": "jlSetkfrjtmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "spark = SparkSession.builder.appName(\"WriteToFile\").getOrCreate()\n",
        "df = spark.readStream.format(\"rate\").load()\n",
        "query = df.writeStream \\\n",
        "    .format(\"parquet\") \\\n",
        "    .option(\"path\", \"/content/output/\") \\\n",
        "    .option(\"checkpointLocation\", \"/content/checkpoint/\") \\\n",
        "    .start()\n",
        "query.awaitTermination(5)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "C4HfRbAijvTM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}